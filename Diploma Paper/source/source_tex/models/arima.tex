\subsubsection{Auto-regressive Integrated Moving Average}
	Логичным предположением является тот факт, что значения конкретного временного ряда зависят от своих предыдущих значений, как было показано на примере EWMA (\myref{link::ewma}). Обобщая данную мысль, текущее ($t$) значение временного ряда может зависеть от некоторого на $k$ шагов отстающего его значения ($t - k$). Тогда получается формула вида:
	\begin{equation}
		y_t = \alpha_0 + \sum_{p = 1}^{q} \alpha_p \cdot y_p + \varepsilon_t: \; \varepsilon_t \sim N(0,\sigma^2)
	\end{equation}
	Пока что никаких ограничений на $\alpha_p$ не накладывается. \textbf{Q}: Но что такое переход от $y_t$ элементу ряда к $y_{t - 1}$? \textbf{A}: Это применение некоторого оператора лага (Lag/Backshift) $B: By_t = y_{t - 1}, B^2y_t = y_{t - 2}$ и так далее. \textbf{Q}: Что должен представлять из себя данный оператор? \textbf{A}: 1) Он должен быть линеен, то есть: $B(x_t + k \cdot y_t) = Bx_t + k \cdot By_t = x_{t - 1} + k \cdot y_{t - 1}: k \in \R$. \textbf{Q}: Как грамотно задать данный оператор? \textbf{A}: Представляем ситуацию, в которой необходимо из вектора $Y_n = (y_1, \ldots, y_n)^T$ получить вектор $Y_{n - 1} =  (0, y_1, \ldots, y_{n - 1})^T$, тогда матрица $B: \R^{n \times 1} \to \R^{n \times 1}$ имеет вид.
	\begin{equation}
		\left(\begin{matrix}
			0 & 0 & \cdots & 0\\
			1 & 0 & & 0\\
			\vdots & \ddots & \ddots & \vdots\\
			0 & \cdots & 1 & 0
		\end{matrix}\right) 
		\left(\begin{matrix}
			y_1\\ y_2\\ y_3 \\ \vdots \\ y_n
		\end{matrix}\right) = 
		\left(\begin{matrix}
			0\\ y_1\\ y_2 \\ \vdots \\ y_{n - 1}
		\end{matrix}\right)
	\end{equation}
	Во-первых, она квадратная, во-вторых интересно, что данный оператор является нильпотентным степени $n$. Из этого следует, что a) единственное собственное значение $B$ это $0$ b) $B^n = O$ (нулевая матрица), то есть степень нильпотентности не превосходит $n$. Более подробно в работе \cite{panov_linear_operator}. Но прежде, чем переписать полученную выше сумму в надлежащем виде, вспоминаем о возможной зависимости $y_t$ от предыдущих значений скользящей средней ($\varepsilon_{t - l}$), где $\varepsilon_{t - l}$ - значение, полученное из белого шума \footnote{Белый шум - независимые, взятые из одного распределения величины - часто $N(0,\sigma^2)$.}.
	\begin{equation}
		\begin{split}
			y_t & = \alpha_0 + \sum_{k = 1}^{p} \alpha_k \cdot B^k y_{t} + \sum_{j = 1}^{q} \beta_j \cdot B^j \varepsilon_{t} + \varepsilon_t\\
			\left(1 - \sum_{k = 1}^{p} \alpha_k \cdot B^k\right) y_t & = \alpha_0 + \left(\sum_{j = 1}^{q} \beta_j \cdot B^j\right) \varepsilon_{t}
		\end{split}
	\end{equation}
	При этом $B\alpha_0 = \alpha_0: \alpha_0 \in \R$ по построению. Таким образом, получаем модель авторегрессионной скользящей средней ARMA$(p,q)$. Но по первоначальной предпосылке о стационарности \myref{def::weak_ts_stationarity}, есть желание обобщить модель на случай нестационарных рядов. То есть сделать ее более универсальной. Для этого формально выводим условие стационарности. Выражению $1 - \sum_{k = 1}^{p} \alpha_k B^k$ в соответствие ставим характеристическое уравнение $1 - \sum_{k = 1}^{p} \alpha_k z^k$. Тогда если все корни данного уравнения (в силу Основной теоремы Алгебры, полученный полином над комплексной плоскостью имеет не более, чем $p$ различных корней) $|z_i| > 1$, то данный ряд стационарен. Подробнее для AR(1):
	\begin{equation}
		\begin{split}
			y_t = \alpha \cdot y_{t - 1} + \varepsilon_t \Rightarrow 1 - \alpha \cdot z = 0 \Rightarrow z = \frac{1}{\alpha}\\
			y_{t + 1} - \alpha \cdot y_{t} \approx 0 \Rightarrow \lambda - \alpha = 0 \Rightarrow y_t \approx c \cdot \alpha^t: \; c \in \R 
		\end{split}
	\end{equation}
	Получается, чтобы числовые значения данного ряда не уходили в $\infty$ при $t \to \infty$ необходимо, чтобы в случае AR$(1) \; |z| > 1$, а $|\alpha| < 1$. Аналогично для случаев AR$(p)$. Таким образом, вывод: необходим множитель, который помогал бы последовательности переходить к $d$ - ому порядку интегрирования. \textbf{Q}: Как это сделать? \textbf{A}: $\nabla^1 y_t = y_t - y_{t - 1} = y_t - By_t = (1 - B) y_t, \nabla^2y_t = \nabla^1y_t - \nabla^1 y_{t - 1} = (1 - B) y_t - (1 - B) y_{t - 1} = = (1 - B) y_t - (1 - B) By_t = (1 - 2B + B^2) y_t = (1 - B)^2 y_t$ и так далее. Получается, что необходимый множитель: $\nabla^d = (1 - B)^d$. Соответственно формула ARIMA$(p,d,q)$ имеет вид:
	\begin{equation}
		\underbrace{\left(1 - \sum_{k = 1}^{p} \alpha_k \cdot B^k\right) \overbrace{(1 - B)^d}^{\text{I}(d)} y_t}_{\text{AR}(p)} = \overbrace{\alpha_0}^{\text{const}} + \underbrace{\left(\sum_{j = 1}^{q} \beta_j \cdot B^j\right) \varepsilon_{t}}_{\text{MA}(q)}
	\end{equation}
	Получившаяся модель включает в себя авторегрессионную составляющую порядка $p$, скользящую среднюю порядка $q$ и интегрированность порядка $d$. Также стоит отметить, что любой стационарный процесс можно разложить в бесконечный (абсолютно сходящийся) процесс скользящего среднего. Более известен данный факт под названием - разложение Вольда \cite{wold_decomposition}. Для примера предполагаем, что некоторый процесс AR$(1)$ стационарен.
	\begin{equation}
		\begin{split}
			y_t & = \alpha_ 0 + \alpha \cdot y_{t - 1} + \varepsilon_{t}\\
			(1 - \alpha B) \cdot y_t & = \alpha_0 + \varepsilon_{t}\\
			(1 - \alpha B)^{-1}(1 - \alpha B) y_t & = (1 - \alpha B)^{-1} (\alpha_0 + \varepsilon_{t})\\
			y_t & = (1 - \alpha B)^{-1} \cdot \alpha_0 + (1 - \alpha B)^{-1} \cdot \varepsilon_{t}\\
			\text{Но ряд стационарен} \Rightarrow y_t & = \sum_{j = 1}^{\infty} B^j\alpha_0 + \sum_{j = 1}^{\infty} B^j\varepsilon_{t} = \frac{\alpha_0}{1 - \alpha} + \sum_{j = 1}^{\infty} \alpha^j \cdot B^j\varepsilon_{t}
		\end{split}
	\end{equation}
	Более подробно о данной модели написано в \cite{arima}. Процесс обучения (то есть подбора соответствующих коэффициентов модели) происходит посредством применения ММП (Метода Максимального Правдоподобия), максимизирующего вероятность наиболее точно описать входные данные. Глобальная задача, стоящая пред алгоритмом подбора параметров (читать далее - алгоритма обучения) это:
	\begin{equation}
		\sum_{p}^n e_p^2
	\end{equation}
	Где $e_p = y_p - \hat{y}_p$ - остаток (разница между предсказанным и исходным значением). Это очень похоже на Метода Наименьших Квадратов (МНК), только МНК дает аналитическую формулу оценок коэффициентов, а в общем случае это не всегда возможно, поэтому для подобных задач часто применяется ММП. Тут же встает вопрос: \textbf{Q}: Как понять, какая модель лучше описывает данные? \textbf{A}: 1) Критерий Акаике (AIC):
	\begin{equation}
		J_{\text{AIC}} = 2k + n \cdot \ln\left[\frac{1}{n} \sum_{p = 1}^n \left(y_p- \hat{y}_p \right)^2\right]
	\end{equation}
	Где $k$ - количество параметров в обучаемой модели, $n$ - общее количество наблюдений, $(y_p - \hat{y}_p)^2 = e_p^2$ - остаток от регрессии 2) Байесовский информационный критерий (BIC):
	\begin{equation}
	 J_{\text{BIC}} = \frac{1}{n} \sum_{p = 1}^n \left(y_p - \hat{y}_p \right)^2 + \frac{k \hat{\sigma}^2 \ln(n)}{n}
	\end{equation} 
	Где $\hat{\sigma}^2$ - оценка (так как реальное значение неизвестно) дисперсии шума ряда, полученного по формуле $(y - \hat{y})$. \cite{alexandridis2014wavelet}
	Очевидно, что с помощью данной можно предсказывать последующие значения числовой последовательности, поэтому данная модель также включается в список сравнительной таблицы.